{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Project\n",
    "### 1. Introduction\n",
    "Sentiment analysis is a natural language processing (NLP) task that involves determining the sentiment expressed in a piece of text, such as whether it is positive, negative, or neutral. This application is widely used in industries to analyze customer feedback, reviews, social media posts, and much more. For example, businesses use sentiment analysis to gauge customer satisfaction or identify negative feedback for timely action.\n",
    "\n",
    "The purpose of this project is to walk through the essential stages of implementing sentiment analysis using Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Import Necessary Libraries\n",
    "# These libraries are required for data handling, preprocessing, feature extraction, and modeling.\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gmwende\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Gmwende\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Gmwende\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded NLTK resources are essential for text preprocessing in sentiment analysis. The stopwords dataset contains common words like 'the' and 'and' which are removed to reduce noise and focus on meaningful content. wordnet is a lexical database used with the WordNetLemmatizer to normalize words by converting them to their root forms, ensuring consistency (e.g., \"running\" → \"run\"). The punkt tokenizer helps split text into sentences or words, enabling token-level operations like removing stopwords and extracting features. These resources streamline preprocessing, making raw text structured and ready for machine learning models, ensuring accurate and efficient analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            label                                               text\n",
      "1599949  positive  OMG how good is ben and jerrys cookie dough ic...\n",
      "1599950  positive  oooo haha just waking up and ready to eat a de...\n",
      "1599951  positive  #Traveltuesday @GuyNGirlTravels Because their ...\n",
      "1599952  positive  any ideaZ on what to get dad for father's day ...\n",
      "1599953  positive  God works mysteriously!i learn that if u think...\n",
      "1599954  positive  @_CrC_ mornin.. I'm enjoying a beautiful morni...\n",
      "1599955  positive  Woke up feeling rested and refreshed today! It...\n",
      "1599956  positive  @naijagal You just HAD to throw that in. Tell ...\n",
      "1599957  positive  @siovene lol I don't blame you it's not the sa...\n",
      "1599958  positive  @ashinynewcoin yeah, that'd be the one  sorry ...\n",
      "1599959  positive     @pokapolas love the donut and the toadstool.  \n",
      "1599960  positive  @crgrs359 Skip the aquarium and check out thes...\n",
      "1599961  positive           @GroleauNET Yeah I'm being an ass today \n",
      "1599962  positive                           @OHTristaN it's sunoudy \n",
      "1599963  positive                     @kbonded Newsflash: It worked \n",
      "1599964  positive  @stum450n Hi. Thanks for the follow. Nice webs...\n",
      "1599965  positive  got home an hour ago ate lunch watched some tv...\n",
      "1599966  positive                                  checking my mail \n",
      "1599967  positive   done la examen! easy peasy  so proud of myself!!\n",
      "1599968  positive  @davepell you're the undisputed authority on t...\n",
      "1599969  positive  Thanks @eastwestchic &amp; @wangyip Thanks! Th...\n",
      "1599970  positive  @marttn thanks Martin. not the most imaginativ...\n",
      "1599971  positive          @MikeJonesPhoto Congrats Mike  Way to go!\n",
      "1599972  positive  http://twitpic.com/7jp4n - OMG! Office Space.....\n",
      "1599973  positive  @yrclndstnlvr ahaha nooo you were just away fr...\n",
      "1599974  positive  @BizCoachDeb  Hey, I'm baack! And, thanks so m...\n",
      "1599975  positive  @mattycus Yeah, my conscience would be clear i...\n",
      "1599976  positive  @MayorDorisWolfe Thats my girl - dishing out t...\n",
      "1599977  positive                          @shebbs123 i second that \n",
      "1599978  positive                                     In the garden \n",
      "1599979  positive  @myheartandmind jo jen by nemuselo zrovna tÃ© ...\n",
      "1599980  positive  Another Commenting Contest! [;: Yay!!!  http:/...\n",
      "1599981  positive  @thrillmesoon i figured out how to see my twee...\n",
      "1599982  positive  @oxhot theri tomorrow, drinking coffee, talkin...\n",
      "1599983  positive  You heard it here first -- We're having a girl...\n",
      "1599984  positive  if ur the lead singer in a band, beware fallin...\n",
      "1599985  positive              @tarayqueen too much ads on my blog. \n",
      "1599986  positive  @La_r_a NEVEER  I think that you both will get...\n",
      "1599987  positive  @Roy_Everitt ha- good job. that's right - we g...\n",
      "1599988  positive                 @Ms_Hip_Hop im glad ur doing well \n",
      "1599989  positive                              WOOOOO! Xbox is back \n",
      "1599990  positive  @rmedina @LaTati Mmmm  That sounds absolutely ...\n",
      "1599991  positive                  ReCoVeRiNg FrOm ThE lOnG wEeKeNd \n",
      "1599992  positive                                  @SCOOBY_GRITBOYS \n",
      "1599993  positive  @Cliff_Forster Yeah, that does work better tha...\n",
      "1599994  positive  Just woke up. Having no school is the best fee...\n",
      "1599995  positive  TheWDB.com - Very cool to hear old Walt interv...\n",
      "1599996  positive  Are you ready for your MoJo Makeover? Ask me f...\n",
      "1599997  positive  Happy 38th Birthday to my boo of alll time!!! ...\n",
      "1599998  positive  happy #charitytuesday @theNSPCC @SparksCharity...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Load Dataset\n",
    "data = pd.read_csv(\"data/sentiment_dataset.csv\")  # Replace with actual file path\n",
    "print(data.tail(50))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Initialize lemmatizer and stop words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace non-alphanumeric characters, but retain numbers\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', text)  # Keep alphabets, numbers, and spaces only\n",
    "    # print(f'text,{text}')\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # print(f'tokens,{tokens}')\n",
    "    # Remove stopwords\n",
    "    cleaned_tokens = [word for word in tokens if word not in stop_words]\n",
    "    # print(f'cleaned_tokens,{cleaned_tokens}')\n",
    "    # lemmatize tokens\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in cleaned_tokens]\n",
    "    # print(f'lemmatized_tokens,{lemmatized_tokens}')\n",
    "    # Return the processed text as a single string\n",
    "    return ' '.join(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess_text(\"The directions for taking out a link are a bit confusing, and $50 I have\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # review = \"50$\"\n",
    "# preprocess_text(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `preprocess_text` function is a comprehensive text-cleaning utility essential for preparing raw text data in sentiment analysis tasks. Initially, it initializes two key components: a lemmatizer from NLTK’s `WordNetLemmatizer` and a set of English stopwords from NLTK. The first step in preprocessing is converting the entire text to lowercase, ensuring that the model treats words like \"Apple\" and \"apple\" equally. Next, the function removes non-alphanumeric characters, including punctuation, using regular expressions, which helps to focus on meaningful content while retaining numbers for context. The text is then tokenized into individual words using NLTK’s `word_tokenize`, allowing for granular manipulation. Stopwords—commonly used words that don’t add significant meaning—are filtered out to reduce noise and enhance the model’s ability to identify relevant words. Following this, lemmatization is applied to each token using the `WordNetLemmatizer`, which converts words to their root forms (e.g., \"running\" becomes \"run\"), standardizing variations. Finally, the processed tokens are reassembled into a single string, ready for input into machine learning models. This function plays a pivotal role in transforming raw, unstructured text into a clean and meaningful format that improves model performance and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply preprocessing to the dataset\n",
    "data = data.dropna()\n",
    "data['label'] = data['label'].map({'negative': 0, 'positive': 1})\n",
    "data['text'] = data['text'].apply(preprocess_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code above drops null values from that data, then maps the values negative and positive to 0 and 1 respectively.\n",
    "\n",
    "The next line of code uses the function we created abouve to preprocess our data, thus in the end we will have cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>upset update facebook texting might cry result...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>kenichan dived many time ball managed save 50 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>whole body feel itchy like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>nationwideclass behaving mad see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>kwesidei whole crew</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                               text\n",
       "0      0  upset update facebook texting might cry result...\n",
       "1      0  kenichan dived many time ball managed save 50 ...\n",
       "2      0                    whole body feel itchy like fire\n",
       "3      0                   nationwideclass behaving mad see\n",
       "4      0                                kwesidei whole crew"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>1</td>\n",
       "      <td>woke school best feeling ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1</td>\n",
       "      <td>thewdb com cool hear old walt interview http b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1</td>\n",
       "      <td>ready mojo makeover ask detail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1</td>\n",
       "      <td>happy 38th birthday boo alll time tupac amaru ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1</td>\n",
       "      <td>happy charitytuesday thenspcc sparkscharity sp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                               text\n",
       "1599994      1                      woke school best feeling ever\n",
       "1599995      1  thewdb com cool hear old walt interview http b...\n",
       "1599996      1                     ready mojo makeover ask detail\n",
       "1599997      1  happy 38th birthday boo alll time tupac amaru ...\n",
       "1599998      1  happy charitytuesday thenspcc sparkscharity sp..."
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data Into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 5. Split Dataset\n",
    "# Split into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data['text'], data['label'], test_size=0.2, random_state=42, shuffle=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we split our data into train and test data for model training and evaluation purposes. \n",
    "\n",
    "80% train data and 20% test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Build Pipeline\n",
    "# Create a pipeline to streamline feature extraction and model training.\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a pipepline, to streamline feature extraction and model training. TfidVectorizer as we had earlier discussed quantifies the weight of a token in a given text. (like stardadizing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [1000, 3000, 5000],\n",
    "    'tfidf__ngram_range': [(1, 1), (1, 2)],  # Unigrams or Unigrams + Bigrams\n",
    "    'model__C': [0.1, 1, 10],  # Regularization strength\n",
    "    'model__solver': ['liblinear', 'lbfgs'],  # Solver for Logistic Regression\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
       "                                       (&#x27;model&#x27;, LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;model__C&#x27;: [0.1, 1, 10],\n",
       "                         &#x27;model__solver&#x27;: [&#x27;liblinear&#x27;, &#x27;lbfgs&#x27;],\n",
       "                         &#x27;tfidf__max_features&#x27;: [1000, 3000, 5000],\n",
       "                         &#x27;tfidf__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
       "                                       (&#x27;model&#x27;, LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;model__C&#x27;: [0.1, 1, 10],\n",
       "                         &#x27;model__solver&#x27;: [&#x27;liblinear&#x27;, &#x27;lbfgs&#x27;],\n",
       "                         &#x27;tfidf__max_features&#x27;: [1000, 3000, 5000],\n",
       "                         &#x27;tfidf__ngram_range&#x27;: [(1, 1), (1, 2)]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=2)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()), (&#x27;model&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                       ('model', LogisticRegression())]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'model__C': [0.1, 1, 10],\n",
       "                         'model__solver': ['liblinear', 'lbfgs'],\n",
       "                         'tfidf__max_features': [1000, 3000, 5000],\n",
       "                         'tfidf__ngram_range': [(1, 1), (1, 2)]},\n",
       "             scoring='accuracy', verbose=2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "# Perform GridSearchCV\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'model__C': 1, 'model__solver': 'liblinear', 'tfidf__max_features': 5000, 'tfidf__ngram_range': (1, 2)}\n",
      "Best Cross-Validation Accuracy: 0.7710498213585552\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Output the best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.77169375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.75      0.77    159494\n",
      "           1       0.76      0.80      0.78    160506\n",
      "\n",
      "    accuracy                           0.77    320000\n",
      "   macro avg       0.77      0.77      0.77    320000\n",
      "weighted avg       0.77      0.77      0.77    320000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on a test set\n",
    "\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "print(\"Test Set Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Predict Sentiment for Custom Reviews\n",
    "# Function to predict sentiment for a given review.\n",
    "def predict_sentiment(review):\n",
    "    review_preprocessed = preprocess_text(review)\n",
    "    print(review_preprocessed)\n",
    "    prediction = grid_search.best_estimator_.predict([review_preprocessed])\n",
    "    print(prediction)\n",
    "    return \"Positive\" if prediction[0] == 1 else \"Negative\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Negative\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example:\n",
    "review = \"Our president is overtaxing us, which is becoming so burdensome to many kenyans who are not employed\"\n",
    "\n",
    "print(\"Prediction:\", predict_sentiment(review))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
